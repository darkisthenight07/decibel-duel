{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_Ua8_tTv4Icl",
        "outputId": "77fa1a36-45e2-4535-dcab-5cd06a0e33f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchaudio torchvision transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g05qSEif4JYF"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 0. IMPORTS & INITIAL SETUP\n",
        "# ==============================================================================\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.autograd as autograd\n",
        "import torch.cuda.amp as amp  # For Mixed Precision\n",
        "import torchvision.utils as vutils # For saving image grids\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Optimizations ---\n",
        "torch.backends.cudnn.benchmark = True # cuDNN speedup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaI4oIe94Ndl",
        "outputId": "7a24ebd8-5df6-4265-9207-74a8e86fde2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "boIIOEtU4QGj"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 2. GAN MODEL DEFINITIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "class WGAN_Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, num_categories, spec_shape=(128, 512), embed_dim=16):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_categories = num_categories\n",
        "        self.spec_shape = spec_shape\n",
        "        H, W = spec_shape\n",
        "\n",
        "        self.start_h, self.start_w = H // 16, W // 16\n",
        "\n",
        "        self.label_emb = nn.Embedding(num_categories, embed_dim)\n",
        "\n",
        "        self.z_proj = nn.Linear(latent_dim, 256 * self.start_h * self.start_w)\n",
        "        self.emb_proj = nn.Linear(embed_dim, 256 * self.start_h * self.start_w)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, y_idx):\n",
        "        z_proj = self.z_proj(z).view(-1, 256, self.start_h, self.start_w)\n",
        "        emb = self.label_emb(y_idx.argmax(dim=1))\n",
        "        emb_proj = self.emb_proj(emb).view(-1, 256, self.start_h, self.start_w)\n",
        "\n",
        "        x = torch.cat([z_proj, emb_proj], dim=1)\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6YdVVqZF4g2f"
      },
      "outputs": [],
      "source": [
        "class WGAN_Critic(nn.Module):\n",
        "    def __init__(self, num_categories, spec_shape=(128, 512), embed_dim=16):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.spec_shape = spec_shape\n",
        "        self.H, self.W = spec_shape\n",
        "\n",
        "        self.label_emb = nn.Embedding(num_categories, embed_dim)\n",
        "        self.emb_proj = nn.Linear(embed_dim, 1 * self.H * self.W)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(2, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 1, kernel_size=(self.H // 16, self.W // 16), stride=1, padding=0)\n",
        "        )\n",
        "\n",
        "    def forward(self, spec, y_idx):\n",
        "        emb = self.label_emb(y_idx.argmax(dim=1))\n",
        "        emb_map = self.emb_proj(emb).view(-1, 1, self.H, self.W)\n",
        "\n",
        "        x = torch.cat([spec, emb_map], dim=1)\n",
        "        x = self.net(x)\n",
        "        return x.view(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "z4ymPIZE4ng2"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 3. UTILITY FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_audio_gan(generator, category_idx, num_samples, device, sample_rate=22050):\n",
        "    generator.eval()\n",
        "    num_categories = generator.num_categories\n",
        "    latent_dim = generator.latent_dim\n",
        "\n",
        "    y = F.one_hot(torch.tensor([category_idx] * num_samples), num_classes=num_categories).float().to(device)\n",
        "    z = torch.randn(num_samples, latent_dim, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with amp.autocast(enabled=(device == 'cuda')):\n",
        "            log_spec_gen = generator(z, y)\n",
        "\n",
        "    spec_gen = torch.expm1(log_spec_gen)\n",
        "    spec_gen = spec_gen.squeeze(1)\n",
        "\n",
        "    spec_gen = spec_gen.float()\n",
        "    inverse_mel = torchaudio.transforms.InverseMelScale(\n",
        "        n_stft=1024 // 2 + 1, n_mels=128, sample_rate=sample_rate\n",
        "    ).to(device)\n",
        "    linear_spec = inverse_mel(spec_gen)\n",
        "\n",
        "    griffin = torchaudio.transforms.GriffinLim(\n",
        "        n_fft=1024, hop_length=256, win_length=1024, n_iter=32\n",
        "    ).to(device)\n",
        "\n",
        "    waveform = griffin(linear_spec)\n",
        "    return waveform.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9cl8f5HE4016"
      },
      "outputs": [],
      "source": [
        "def save_and_play(wav, sample_rate, filename):\n",
        "    if wav.dim() > 2: wav = wav.squeeze(0)\n",
        "    torchaudio.save(filename, wav, sample_rate=sample_rate)\n",
        "    print(f\"Saved to {filename}\")\n",
        "    display(Audio(data=wav.numpy(), rate=sample_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6HIzXy-841wd"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 3.1 Checkpointing\n",
        "# ==============================================================================\n",
        "def save_checkpoint(g, c, g_optim, c_optim, epoch, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'g_state_dict': g.state_dict(),\n",
        "        'c_state_dict': c.state_dict(),\n",
        "        'g_optim_state_dict': g_optim.state_dict(),\n",
        "        'c_optim_state_dict': c_optim.state_dict(),\n",
        "    }\n",
        "    torch.save(state, path)\n",
        "    print(f\"Checkpoint saved to {path}\")\n",
        "\n",
        "def load_checkpoint(g, c, g_optim, c_optim, path, device):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"No checkpoint found at {path}. Starting from scratch.\")\n",
        "        return 0\n",
        "\n",
        "    state = torch.load(path, map_location=device)\n",
        "    g.load_state_dict(state['g_state_dict'])\n",
        "    c.load_state_dict(state['c_state_dict'])\n",
        "    g_optim.load_state_dict(state['g_optim_state_dict'])\n",
        "    c_optim.load_state_dict(state['c_optim_state_dict'])\n",
        "    start_epoch = state['epoch'] + 1\n",
        "    print(f\"Loaded checkpoint from {path}. Resuming at epoch {start_epoch}.\")\n",
        "    return start_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bbaeaYCd5JqO"
      },
      "outputs": [],
      "source": [
        "def compute_gradient_penalty(critic, real_specs, fake_specs, labels, device, gp_weight=10.0):\n",
        "    B = real_specs.size(0)\n",
        "    alpha = torch.rand(B, 1, 1, 1, device=device)\n",
        "\n",
        "    interpolated = (alpha * real_specs + (1 - alpha) * fake_specs).requires_grad_(True)\n",
        "\n",
        "    c_interpolated = critic(interpolated, labels)\n",
        "\n",
        "    grad_outputs = torch.ones(c_interpolated.size(), device=device, requires_grad=False)\n",
        "    gradients = autograd.grad(\n",
        "        outputs=c_interpolated,\n",
        "        inputs=interpolated,\n",
        "        grad_outputs=grad_outputs,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(B, -1)\n",
        "    gradient_norm = gradients.norm(2, dim=1)\n",
        "    gradient_penalty = gp_weight * ((gradient_norm - 1) ** 2).mean()\n",
        "    return gradient_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "d0LinDwZ5LdE"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 4. GAN TRAINING FUNCTION\n",
        "# ==============================================================================\n",
        "def train_wgan_gp(\n",
        "    generator, critic, dataloader, device, categories, epochs, lr, betas,\n",
        "    latent_dim, n_critic, gp_weight, use_amp, checkpoint_path, sample_dir\n",
        "):\n",
        "    optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
        "    optimizer_D = optim.Adam(critic.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "    scaler = amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "    os.makedirs(sample_dir, exist_ok=True)\n",
        "    os.makedirs(\"gan_generated_audio\", exist_ok=True)\n",
        "    os.makedirs(\"gan_spectrogram_plots\", exist_ok=True)\n",
        "\n",
        "    start_epoch = load_checkpoint(\n",
        "        generator, critic, optimizer_G, optimizer_D, checkpoint_path, device\n",
        "    )\n",
        "\n",
        "    fixed_noise = torch.randn(len(categories), latent_dim, device=device)\n",
        "    fixed_labels = F.one_hot(torch.arange(len(categories)), num_classes=len(categories)).float().to(device)\n",
        "\n",
        "    for epoch in range(start_epoch, epochs + 1):\n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
        "        for real_specs, labels in loop:\n",
        "            real_specs = real_specs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = real_specs.size(0)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Critic\n",
        "            # ---------------------\n",
        "            for _ in range(n_critic):\n",
        "                optimizer_D.zero_grad()\n",
        "                noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "\n",
        "                with amp.autocast(enabled=use_amp):\n",
        "                    with torch.no_grad():\n",
        "                        fake_specs = generator(noise, labels).detach()\n",
        "\n",
        "                    real_output = critic(real_specs, labels)\n",
        "                    fake_output = critic(fake_specs, labels)\n",
        "                    gp = compute_gradient_penalty(critic, real_specs, fake_specs, labels, device, gp_weight)\n",
        "                    loss_D = fake_output.mean() - real_output.mean() + gp\n",
        "\n",
        "                scaler.scale(loss_D).backward()\n",
        "                scaler.step(optimizer_D)\n",
        "                scaler.update()\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            with amp.autocast(enabled=use_amp):\n",
        "                noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "                fake_specs = generator(noise, labels)\n",
        "                fake_output = critic(fake_specs, labels)\n",
        "                loss_G = -fake_output.mean()\n",
        "\n",
        "            scaler.scale(loss_G).backward()\n",
        "            scaler.step(optimizer_G)\n",
        "            scaler.update()\n",
        "\n",
        "            loop.set_postfix(D_Loss=loss_D.item(), G_Loss=loss_G.item(), GP=gp.item())\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "          # --- End of Epoch: Generate and save samples ---\n",
        "          print(f\"\\n--- Generating Samples for Epoch {epoch} ---\")\n",
        "          generator.eval()\n",
        "\n",
        "          with torch.no_grad():\n",
        "              with amp.autocast(enabled=use_amp):\n",
        "                  fake_specs_grid = generator(fixed_noise, fixed_labels).cpu()\n",
        "              vutils.save_image(\n",
        "                  fake_specs_grid,\n",
        "                  os.path.join(sample_dir, f\"epoch_{epoch:03d}.png\"),\n",
        "                  nrow=len(categories),\n",
        "                  normalize=True,\n",
        "                  value_range=(0, fake_specs_grid.max().item())\n",
        "              )\n",
        "\n",
        "          fig, axes = plt.subplots(1, len(categories), figsize=(4 * len(categories), 4))\n",
        "          if len(categories) == 1: axes = [axes]\n",
        "          for cat_idx, cat_name in enumerate(categories):\n",
        "              spec_gen_log_np = fake_specs_grid[cat_idx].squeeze().numpy()\n",
        "              axes[cat_idx].imshow(spec_gen_log_np, aspect='auto', origin='lower', cmap='viridis')\n",
        "              axes[cat_idx].set_title(f'{cat_name} (Epoch {epoch})')\n",
        "              axes[cat_idx].axis('off')\n",
        "          plt.tight_layout()\n",
        "          plt.savefig(os.path.join(sample_dir, f\"epoch_gan_spectrogram_plot_{epoch:03d}.png\"))\n",
        "          plt.show()\n",
        "          plt.close(fig)\n",
        "\n",
        "          for cat_idx, cat_name in enumerate(categories):\n",
        "              wav = generate_audio_gan(generator, cat_idx, 1, device)\n",
        "              fname = os.path.join(sample_dir,f\"gan_generated_audio/{cat_name}_ep{epoch}.wav\")\n",
        "              save_and_play(wav, sample_rate=22050, filename=fname)\n",
        "\n",
        "        generator.train()\n",
        "\n",
        "        save_checkpoint(\n",
        "            generator, critic, optimizer_G, optimizer_D, epoch, checkpoint_path\n",
        "        )\n",
        "        print(\"--- End of Epoch ---\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aX7TDCNE53Go"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "class PrecomputedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This dataset is extremely fast. It just loads pre-computed\n",
        "    spectrograms from .pt files.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, categories):\n",
        "        self.root_dir = root_dir\n",
        "        self.categories = categories\n",
        "        self.file_list = []\n",
        "\n",
        "        print(\"Populating file list from pre-computed dataset...\")\n",
        "        for i, category in enumerate(self.categories):\n",
        "            path = os.path.join(self.root_dir, category)\n",
        "            if not os.path.isdir(path):\n",
        "                print(f\"Warning: Directory not found: {path}\")\n",
        "                continue\n",
        "\n",
        "            # Find all pre-computed .pt files\n",
        "            files = glob.glob(os.path.join(path, \"*.pt\"))\n",
        "            self.file_list.extend(files)\n",
        "\n",
        "        print(f\"Found {len(self.file_list)} pre-computed spectrograms.\")\n",
        "        if len(self.file_list) == 0:\n",
        "            print(\"\\n*** WARNING: No .pt files found! ***\")\n",
        "            print(f\"Did you run the pre-computation cell and set the correct PRECOMPUTED_PATH?\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the saved tensor pair (spec, label)\n",
        "        try:\n",
        "            spec, label = torch.load(self.file_list[idx])\n",
        "            return spec, label\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError loading pre-computed file {self.file_list[idx]}: {e}\")\n",
        "            # Return a dummy sample that matches shape\n",
        "            return torch.zeros(1, 128, 256), F.one_hot(torch.tensor(0), num_classes=len(self.categories)).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZrvgya8584z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ba527f-0776-46eb-f629-f91db73a84f6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Found 5 categories: ['dog_bark', 'drilling', 'engine_idling', 'siren', 'street_music']\n",
            "Spectrogram Shape: (128, 256)\n",
            "Batch Size: 64\n",
            "Populating file list from pre-computed dataset...\n",
            "Found 3450 pre-computed spectrograms.\n",
            "Loaded checkpoint from /content/drive/MyDrive/decibel_duel/checkpoints/wgan_audio.pth.tar. Resuming at epoch 390.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 390/1000:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 36/53 [07:03<01:51,  6.56s/it, D_Loss=-45.6, GP=14.3, G_Loss=196]"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 6. MAIN EXECUTION BLOCK\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Configuration ---\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    LATENT_DIM = 100\n",
        "    EPOCHS = 1000\n",
        "    SAMPLE_RATE = 22050\n",
        "\n",
        "    # --- WGAN-GP & Optimization Config ---\n",
        "    LEARNING_RATE = 1e-4\n",
        "    BETAS = (0.5, 0.9)\n",
        "    N_CRITIC = 5\n",
        "    GP_WEIGHT = 10\n",
        "    USE_AMP = True\n",
        "\n",
        "    # --- *** OPTIMIZATION: Increased Batch Size *** ---\n",
        "    BATCH_SIZE = 64\n",
        "\n",
        "    # --- *** OPTIMIZATION: Shorter Spectrograms *** ---\n",
        "    MAX_FRAMES = 256 # Was 512, must match pre-computation\n",
        "    SPEC_SHAPE = (128, MAX_FRAMES) # (H, W)\n",
        "\n",
        "    # --- Paths and Data Setup ---\\\n",
        "    BASE_PATH = \"/content/drive/MyDrive/decibel_duel\"\n",
        "    TRAIN_PATH = os.path.join(BASE_PATH, 'train/train')\n",
        "    PRECOMPUTED_PATH = os.path.join(BASE_PATH, 'train/precompute')\n",
        "    CHECKPOINT_PATH = os.path.join(BASE_PATH, 'checkpoints/wgan_audio.pth.tar')\n",
        "    SAMPLE_DIR = os.path.join(BASE_PATH, 'samples')\n",
        "\n",
        "    train_categories = sorted([d for d in os.listdir(TRAIN_PATH) if os.path.isdir(os.path.join(TRAIN_PATH, d))])\n",
        "    NUM_CATEGORIES = len(train_categories)\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"Found {NUM_CATEGORIES} categories: {train_categories}\")\n",
        "    print(f\"Spectrogram Shape: {SPEC_SHAPE}\")\n",
        "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "\n",
        "    train_dataset = PrecomputedDataset(\n",
        "        root_dir=PRECOMPUTED_PATH,\n",
        "        categories=train_categories\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=True # Good for GANs\n",
        "    )\n",
        "\n",
        "    generator = WGAN_Generator(\n",
        "        LATENT_DIM, NUM_CATEGORIES, spec_shape=SPEC_SHAPE\n",
        "    ).to(DEVICE)\n",
        "    critic = WGAN_Critic(\n",
        "        NUM_CATEGORIES, spec_shape=SPEC_SHAPE\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    generator.apply(weights_init)\n",
        "    critic.apply(weights_init)\n",
        "\n",
        "    # --- Start Training ---\\\n",
        "    train_wgan_gp(\n",
        "        generator=generator,\n",
        "        critic=critic,\n",
        "        dataloader=train_loader,\n",
        "        device=DEVICE,\n",
        "        categories=train_categories,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LEARNING_RATE,\n",
        "        betas=BETAS,\n",
        "        latent_dim=LATENT_DIM,\n",
        "        n_critic=N_CRITIC,\n",
        "        gp_weight=GP_WEIGHT,\n",
        "        use_amp=USE_AMP,\n",
        "        checkpoint_path=CHECKPOINT_PATH,\n",
        "        sample_dir=SAMPLE_DIR\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}