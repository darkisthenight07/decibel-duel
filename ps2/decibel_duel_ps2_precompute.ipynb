{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PXPElWMEDXw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6556928f-d3f9-4c17-cb60-898a4632efca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "lUzBA56nDmLJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. DATASET\n",
        "# ==============================================================================\n",
        "\n",
        "class TrainAudioSpectrogramDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This dataset class is used ONCE to pre-process all our audio\n",
        "    into spectrogram tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, categories, max_frames=512, fraction=1.0, sample_rate=22050):\n",
        "        self.root_dir = root_dir\n",
        "        self.categories = categories\n",
        "        self.max_frames = max_frames\n",
        "        self.file_list = []\n",
        "        self.class_to_idx = {cat: i for i, cat in enumerate(categories)}\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=self.sample_rate, n_fft=1024, hop_length=256, n_mels=128\n",
        "        ).to(\"cpu\")\n",
        "\n",
        "        print(\"Populating file list...\")\n",
        "        for i, category in enumerate(self.categories):\n",
        "            path = os.path.join(self.root_dir, category)\n",
        "            if not os.path.isdir(path):\n",
        "                print(f\"Warning: Directory not found: {path}\")\n",
        "                continue\n",
        "\n",
        "            files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(('.wav', '.mp3', '.flac'))]\n",
        "            random.shuffle(files)\n",
        "            files = files[:int(len(files) * fraction)]\n",
        "            for f in files:\n",
        "                self.file_list.append((f, i))\n",
        "        print(f\"Found {len(self.file_list)} audio files.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.file_list[idx]\n",
        "\n",
        "        try:\n",
        "            wav, sr = torchaudio.load(path)\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError loading file {path}: {e}\")\n",
        "            return torch.zeros(1, 128, self.max_frames), F.one_hot(torch.tensor(0), num_classes=len(self.categories)).float()\n",
        "\n",
        "        if sr != self.sample_rate:\n",
        "            wav = torchaudio.functional.resample(wav, sr, self.sample_rate)\n",
        "\n",
        "        if wav.size(0) > 1:\n",
        "            wav = wav.mean(dim=0, keepdim=True)\n",
        "\n",
        "        if wav.shape[1] < 1024:\n",
        "             wav = F.pad(wav, (0, 1024 - wav.shape[1]))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mel_spec = self.mel_transform(wav)\n",
        "\n",
        "        log_spec = torch.log1p(mel_spec)\n",
        "\n",
        "        _, _, n_frames = log_spec.shape\n",
        "        if n_frames < self.max_frames:\n",
        "            pad = self.max_frames - n_frames\n",
        "            log_spec = F.pad(log_spec, (0, pad))\n",
        "        else:\n",
        "            log_spec = log_spec[:, :, :self.max_frames]\n",
        "\n",
        "        label_vec = F.one_hot(torch.tensor(label), num_classes=len(self.categories)).float()\n",
        "        return log_spec, label_vec"
      ],
      "metadata": {
        "id": "fvaXX7CBD8ee"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 5. PRE-COMPUTATION SCRIPT (RUN THIS ONCE)\n",
        "# ==============================================================================\n",
        "def preprocess_and_save():\n",
        "    print(\"Starting dataset pre-computation...\")\n",
        "\n",
        "    BASE_PATH = \"/content/drive/MyDrive/decibel_duel\"\n",
        "    TRAIN_PATH = os.path.join(BASE_PATH, 'train/train')\n",
        "    PRECOMPUTED_PATH = os.path.join(BASE_PATH, 'train/precompute')\n",
        "\n",
        "    NEW_MAX_FRAMES = 256\n",
        "    SAMPLE_RATE = 22050\n",
        "    train_categories = sorted([d for d in os.listdir(TRAIN_PATH) if os.path.isdir(os.path.join(TRAIN_PATH, d))])\n",
        "\n",
        "    print(f\"Output directory: {PRECOMPUTED_PATH}\")\n",
        "    print(f\"Categories: {train_categories}\")\n",
        "    print(f\"Max Frames: {NEW_MAX_FRAMES}\")\n",
        "\n",
        "    # 1. Initialize the *original* dataset class\n",
        "    original_dataset = TrainAudioSpectrogramDataset(\n",
        "        root_dir=TRAIN_PATH,\n",
        "        categories=train_categories,\n",
        "        max_frames=NEW_MAX_FRAMES,\n",
        "        sample_rate=SAMPLE_RATE\n",
        "    )\n",
        "\n",
        "    # 2. Create a simple dataloader to iterate\n",
        "    loader = DataLoader(original_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    # 3. Loop, create save paths, and save\n",
        "    print(f\"Processing {len(original_dataset)} files...\")\n",
        "    for i, (spec, label) in enumerate(tqdm(loader, desc=\"Pre-processing audio\")):\n",
        "        label_idx = label.argmax().item()\n",
        "        category_name = train_categories[label_idx]\n",
        "\n",
        "        save_dir = os.path.join(PRECOMPUTED_PATH, category_name)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        save_path = os.path.join(save_dir, f\"spec_{i:06d}.pt\")\n",
        "\n",
        "        # Squeeze to remove batch_size=1\n",
        "        torch.save((spec.squeeze(0), label.squeeze(0)), save_path)\n",
        "\n",
        "    print(\"--- Pre-computation Complete! ---\")\n",
        "    print(f\"All spectrograms saved to {PRECOMPUTED_PATH}\")\n",
        "\n",
        "# --- Uncomment and run the line below ONCE ---\n",
        "#preprocess_and_save()"
      ],
      "metadata": {
        "id": "9SwhqABVEKH2"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}